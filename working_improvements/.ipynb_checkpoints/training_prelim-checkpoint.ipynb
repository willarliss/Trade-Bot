{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from functions import fetch_data, pickle_model\n",
    "from env import TradingEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = fetch_data('fb_df')\n",
    "df2 = fetch_data('aapl_df')\n",
    "df3 = fetch_data('nflx_df')\n",
    "df4 = fetch_data('goog_df')\n",
    "df5 = fetch_data('sp500_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = {'aapl':df2, 'nflx':df3, 'goog':df4}\n",
    "#stocks = {'aapl':df2}\n",
    "starting_balance = 1_000_000\n",
    "\n",
    "env = TradingEnvironment(stocks, starting_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "start = env.current_step\n",
    "\n",
    "agent_performance = []\n",
    "long_performance = []\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    agent_performance.append(env.agent_portfolio.net_worth)\n",
    "    long_performance.append(env.long_portfolio.net_worth)\n",
    "        \n",
    "    actions = {'aapl':np.random.uniform(-1,1), 'nflx':np.random.uniform(-1,1), 'goog':np.random.uniform(-1,1)}\n",
    "    #actions = {'aapl':1, 'nflx':1, 'goog':1}\n",
    "    #actions = {'aapl':np.random.uniform(-1,1)}\n",
    "\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = env.stocks['aapl'].loc[start:env.current_step-1:-1, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(dates, agent_performance, label='model')\n",
    "plt.plot(dates, long_performance, label='long')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_len):\n",
    "\n",
    "    h1 = 1024\n",
    "    h2 = 584\n",
    "    h3 = 68\n",
    "    eta = 0.01\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add( # Input layer\n",
    "        Flatten(\n",
    "            input_shape=input_shape,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add( # Hidden layer 1\n",
    "        Dense(\n",
    "            units=h1, \n",
    "            activation='relu',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add( # Hidden layer 2\n",
    "        Dense(\n",
    "            units=h2, \n",
    "            activation='relu',\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add( # Hidden layer 3\n",
    "        Dense(\n",
    "            units=h3, \n",
    "            activation='relu',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add( # Output layer\n",
    "        Dense(\n",
    "            units=output_len, \n",
    "            activation='linear',\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=Adam(lr=eta),\n",
    "        )\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self, action_space, state_space,\n",
    "        gamma=0.95, memory_size=100_000, batch_size=100, alpha=1.0, alpha_min=0.01, alpha_decay=0.9, n_epochs=20,\n",
    "        ):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.alpha_min = alpha_min\n",
    "        self.alpha_decay = alpha_decay\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.verbose = 0\n",
    "        self.is_fit = False\n",
    "        self.n_epochs = n_epochs\n",
    "        self.model = build_model(state_space, action_space)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \n",
    "        # Choose random action (i.e. explore environment) depending on exploration rate\n",
    "        if np.random.rand() < self.alpha:\n",
    "            action = np.random.randint(self.action_space)\n",
    "            \n",
    "        # Choose action based on maximum q-value depending on exploration rate\n",
    "        else:\n",
    "            # If nn is fit, predict the q-values (future reward) for each action given the current state\n",
    "            if self.is_fit:\n",
    "                q_values = self.model.predict(\n",
    "                    state.astype(float).reshape(1, *self.state_space)\n",
    "                    )\n",
    "            # If nn is not fit, generate random q_values\n",
    "            else:\n",
    "                q_values = [np.random.randn(self.action_space)]\n",
    "            \n",
    "            # Choose the action based on the greatest \n",
    "            action = np.argmax(q_values[0])\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def remember(self, state, action, reward, state_next, terminal):\n",
    "        \n",
    "        # Commit to memory the current day's state, the action chosen, the reward received for that action,\n",
    "        # what the next state is, and whether or not the action had a terminal outcome\n",
    "        self.memory.append(\n",
    "            (state, action, reward, state_next, terminal)\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def replay(self):\n",
    "        \n",
    "        # If memory is smaller than batch size, experience replay can't be done\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return \n",
    "        \n",
    "        X, y = [], []\n",
    "        # Randomly sample a mini batch from memory to use for training\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "                \n",
    "        # Enumerate each object in the mini batch\n",
    "        for state, action, reward, state_next, terminal in mini_batch:\n",
    "            \n",
    "            # If the current state is terminal, the next state's Q-value is just the current reward\n",
    "            if terminal:\n",
    "                q_update = reward\n",
    "            # If the state is not terminal...\n",
    "            else:\n",
    "                # If the nn is fit, approximate the next state's Q-value\n",
    "                if self.is_fit:\n",
    "                    q_update = reward + self.gamma*np.amax(\n",
    "                        self.model.predict(state_next.astype(float).reshape(1, *self.state_space))[0]\n",
    "                        )\n",
    "                # If the nn is not fit, the next state's Q-value is just the current reward\n",
    "                else:\n",
    "                    q_update = reward\n",
    "            \n",
    "            # If the nn is fit, approximate the current state's Q-values\n",
    "            if self.is_fit:\n",
    "                q_values = self.model.predict(state.astype(float).reshape(1, *self.state_space))\n",
    "            # If the nn is not fit, set the current state's Q-values to be zeros\n",
    "            else:\n",
    "                q_values = np.zeros((1, self.action_space))\n",
    "            \n",
    "            # Update the Q-value of the action chosen at the current state to be the Q-value predicted for the next state\n",
    "            q_values[0][action] = q_update\n",
    "                    \n",
    "            # Add the current observation space and the calculated Q_values to their respective arrays for training\n",
    "            X.append(state)\n",
    "            y.append(q_values[0])\n",
    "            \n",
    "        # Reformat for nn training\n",
    "        X = np.array(X).reshape(self.batch_size, *self.state_space)\n",
    "        y = np.array(y).reshape(self.batch_size, self.action_space)\n",
    "        \n",
    "        # Fit nn using the chosen observation spaces and their calculated Q-values\n",
    "        batch = max(8, int(self.batch_size/8))\n",
    "        self.is_fit = True\n",
    "        self.model.fit(\n",
    "            X.astype(float), \n",
    "            y.astype(float), \n",
    "            batch_size=batch, \n",
    "            epochs=self.n_epochs, \n",
    "            verbose=self.verbose,\n",
    "            )\n",
    "\n",
    "        # Exploration rate decays by set amount\n",
    "        self.alpha = max(self.alpha_min, self.alpha*self.alpha_decay)\n",
    "        \n",
    "        return X, y \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = {'goog':df4}\n",
    "starting_balance = 1_000_000\n",
    "env = TradingEnvironment(stocks, starting_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_actions = np.linspace(-1,1, 5)\n",
    "states = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver = DQN(\n",
    "    action_space=len(_actions),\n",
    "    state_space=env.observation_space.shape,\n",
    "    batch_size=3_000,\n",
    "    memory_size=1_000_000,\n",
    "    alpha=1.0,\n",
    "    alpha_decay=0.999,\n",
    "    alpha_min=0.1,\n",
    "    gamma=0.99,\n",
    "    n_epochs=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              13312     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 584)               598600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 68)                39780     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 345       \n",
      "=================================================================\n",
      "Total params: 652,037\n",
      "Trainable params: 652,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dqn_solver.verbose=1\n",
    "dqn_solver.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "================================================================================================================\n",
      "EPISODE: 7\n",
      "EXECUTION TIME: 5.733696937561035\n",
      "EXPLORATION: 1.0\n",
      "MEMORY SIZE: 2351\n",
      "TOTAL REWARD: -1.596765610657986\n",
      "FINAL PROFIT 852729.6911281061\n",
      "================================================================================================================\n",
      "\t\n",
      "================================================================================================================\n",
      "EPISODE: 8\n",
      "EXECUTION TIME: 5.841412782669067\n",
      "EXPLORATION: 1.0\n",
      "MEMORY SIZE: 4702\n",
      "TOTAL REWARD: -1.9251090169786733\n",
      "FINAL PROFIT 370466.2649694383\n",
      "================================================================================================================\n",
      "\t\n",
      "Train on 3000 samples\n",
      "Epoch 1/20\n",
      "3000/3000 [==============================] - 0s 164us/sample - loss: 59025957577.0000\n",
      "Epoch 2/20\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 167750704.2500\n",
      "Epoch 3/20\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 3603274.3000\n",
      "Epoch 4/20\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 7129.6297\n",
      "Epoch 5/20\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.9620\n",
      "Epoch 6/20\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.9614\n",
      "Epoch 7/20\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.9614\n",
      "Epoch 8/20\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.9597\n",
      "Epoch 9/20\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.9602\n",
      "Epoch 10/20\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.9604\n",
      "Epoch 11/20\n",
      "3000/3000 [==============================] - 0s 58us/sample - loss: 0.9594\n",
      "Epoch 12/20\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.9589\n",
      "Epoch 13/20\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.9590\n",
      "Epoch 14/20\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.9588\n",
      "Epoch 15/20\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.9594\n",
      "Epoch 16/20\n",
      "3000/3000 [==============================] - 0s 71us/sample - loss: 0.9597\n",
      "Epoch 17/20\n",
      "3000/3000 [==============================] - 0s 68us/sample - loss: 0.9593\n",
      "Epoch 18/20\n",
      "3000/3000 [==============================] - 0s 62us/sample - loss: 0.9591\n",
      "Epoch 19/20\n",
      "3000/3000 [==============================] - 0s 61us/sample - loss: 0.9595\n",
      "Epoch 20/20\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.9603\n",
      "================================================================================================================\n",
      "EPISODE: 9\n",
      "EXECUTION TIME: 9.948431015014648\n",
      "EXPLORATION: 0.999\n",
      "MEMORY SIZE: 7053\n",
      "TOTAL REWARD: -1.9238293342533297\n",
      "FINAL PROFIT 703685.2925198057\n",
      "================================================================================================================\n",
      "\t\n",
      "Train on 3000 samples\n",
      "Epoch 1/20\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 1.0128\n",
      "Epoch 2/20\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 50213.2196\n",
      "Epoch 3/20\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.9942\n",
      "Epoch 4/20\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.9932\n",
      "Epoch 5/20\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.9942\n",
      "Epoch 6/20\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 24371.0910\n",
      "Epoch 7/20\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.9929\n",
      "Epoch 8/20\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.9938\n",
      "Epoch 9/20\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.9952\n",
      "Epoch 10/20\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.9922\n",
      "Epoch 11/20\n",
      "3000/3000 [==============================] - ETA: 0s - loss: 0.992 - 0s 43us/sample - loss: 0.9916\n",
      "Epoch 12/20\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.9915\n",
      "Epoch 13/20\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.99150s - loss: 0.983\n",
      "Epoch 14/20\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.9920\n",
      "Epoch 15/20\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.9914\n",
      "Epoch 16/20\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.9916\n",
      "Epoch 17/20\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.9909\n",
      "Epoch 18/20\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.9920\n",
      "Epoch 19/20\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.9913\n",
      "Epoch 20/20\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.9914\n",
      "================================================================================================================\n",
      "EPISODE: 10\n",
      "EXECUTION TIME: 177.4159746170044\n",
      "EXPLORATION: 0.998001\n",
      "MEMORY SIZE: 9404\n",
      "TOTAL REWARD: -1.7734061713693756\n",
      "FINAL PROFIT 1042914.6449550409\n",
      "================================================================================================================\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(6, 2500):\n",
    "    \n",
    "    # Reset the evironment at the top of each episode\n",
    "    state = env.reset()\n",
    "    done = False    \n",
    "    \n",
    "    stock_performance = []\n",
    "    agent_performance = []\n",
    "    long_performance = []\n",
    "    reward_trace = []    \n",
    "    actions = []\n",
    "    \n",
    "    # Perform experience replay (automatically skips if not enough memory)\n",
    "    print('\\t')\n",
    "    start = time.time()\n",
    "    test = dqn_solver.replay()  \n",
    "    \n",
    "    # The model will iterate until a terminal state is reached\n",
    "    while not done:\n",
    "                \n",
    "        # Select an action by passing the current observation/state to the DQN\n",
    "        action = dqn_solver.act(state)\n",
    "        _action = {'goog': _actions[action]}\n",
    "        \n",
    "        # The environment takes a step according to that action and returns the new state, the reward, and the terminal status\n",
    "        next_state, reward, done, info = env.step(_action)\n",
    "                \n",
    "        # Commit to the DQN's memory the relevant information\n",
    "        dqn_solver.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "        \n",
    "        stop = time.time()\n",
    "\n",
    "        actions.append(action)\n",
    "        reward_trace.append(reward)\n",
    "        agent_performance.append(env.agent_portfolio.net_worth)  \n",
    "        long_performance.append(env.long_portfolio.net_worth)\n",
    "            \n",
    "    print('================================================================================================================')\n",
    "    print('EPISODE:', i+1)\n",
    "    print('EXECUTION TIME:', stop - start)\n",
    "    print('EXPLORATION:', dqn_solver.alpha)\n",
    "    print('MEMORY SIZE:', len(dqn_solver.memory))\n",
    "    print('TOTAL REWARD:', np.mean(reward_trace))\n",
    "    print('FINAL PROFIT:', env.net_worth-env.balance_init)\n",
    "    print('================================================================================================================')\n",
    "    \n",
    "    history.append({\n",
    "        'stock': stock_performance,\n",
    "        'agent': agent_performance,\n",
    "        'long': long_performance,\n",
    "        'actions': actions,\n",
    "        'rewards': np.mean(reward_trace),\n",
    "        })\n",
    "\n",
    "X, y = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver.verbose = 1\n",
    "dqn_solver.n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "i = -2\n",
    "X = np.arange(len(history[i]['agent']))\n",
    "\n",
    "ax1.plot(X, history[i]['long'], 'b--', alpha=0.667, label='buy and hold')\n",
    "ax1.plot(X, history[i]['agent'], 'b-', label='backtesting model')\n",
    "ax1.axhline(env.balance_init, alpha=0.333, color='blue')\n",
    "ax1.set_ylabel('portfolio value')\n",
    "ax1.set_title('training')\n",
    "ax1.legend()\n",
    "\n",
    "#ax2.plot(history[i]['stock'], 'r-', label='GOOG')\n",
    "#ax2.set_ylabel('stock value')\n",
    "#ax2.set_xlabel('time')\n",
    "#ax2.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([history[i]['rewards'] for i in np.arange(len(history))])\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_model(dqn_solver, path='base_model_early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

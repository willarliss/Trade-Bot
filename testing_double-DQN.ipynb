{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for testing an embedded Double DQN agent in a continuous state trading environment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from support_code.functions import buy_and_hold, corr, fetch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New environment must be used that can support continuous action space  \n",
    "Define environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, balance_init, actions=9, training=True, train_size=0.8, fee=0.001):\n",
    "        \n",
    "        super(TradingEnv, self).__init__()\n",
    "        assert actions in [5, 7, 9, 11, 21]\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        self.df.sort_values('date', inplace=True, ascending=False)\n",
    "        \n",
    "        self.balance_init = balance_init\n",
    "        self.fee = fee\n",
    "        \n",
    "        self.verbose = 0\n",
    "        self.training = training\n",
    "        self.train_test_split = int(self.df.shape[0]*train_size)\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,))\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(6,5))\n",
    "        \n",
    "    def _next_observation(self):\n",
    "                \n",
    "        self.max_price = np.max(self.df.drop(['date', 'volume'], axis=1).loc[self.current_step:self.first_step-4].values)\n",
    "        self.max_vol = np.max(self.df.loc[self.current_step:self.first_step-4]['volume'])\n",
    "        \n",
    "        obs = self.df.loc[self.current_step: self.current_step-4].copy()\n",
    "        \n",
    "        obs[['open', 'high', 'low', 'close']] /= self.max_price\n",
    "        obs['volume'] /= self.max_vol\n",
    "        \n",
    "        obs = obs.drop('date', axis=1).values\n",
    "        \n",
    "        obs = np.vstack((obs, [ \n",
    "            self.net_worth/self.balance_init, \n",
    "            self.balance/self.balance_init,\n",
    "            self.shares_held,\n",
    "            self.current_step-self.first_step,\n",
    "            self.shares_held/self.df.loc[self.current_step, 'close'],\n",
    "            ]))         \n",
    "\n",
    "        assert obs.shape == self.observation_space.shape\n",
    "        return obs\n",
    "    \n",
    "    def _take_action_verbose(self, action):\n",
    "        \n",
    "        current_price = self.df.loc[self.current_step, 'close']\n",
    "        \n",
    "        print('current price', current_price)\n",
    "        print('action', action)\n",
    "                \n",
    "        if action > 0:\n",
    "            total_possible = self.balance / (current_price*(1+self.fee))\n",
    "            print('total possible to buy', total_possible)\n",
    "            n_shares_bought = total_possible * action\n",
    "            print('number bought', n_shares_bought)\n",
    "                        \n",
    "            cost = n_shares_bought * current_price\n",
    "            cost *= (1 + self.fee)\n",
    "            print('cost of buying', cost)\n",
    "            \n",
    "            self.balance -= cost\n",
    "            self.shares_held += n_shares_bought\n",
    "            print('balance', self.balance, 'shares held', self.shares_held)\n",
    "            \n",
    "        if action < 0:\n",
    "            total_possible = self.shares_held * 1\n",
    "            print('total possible to sell', total_possible)\n",
    "            n_shares_sold = total_possible * -action\n",
    "            print('number sold', n_shares_sold)\n",
    "                        \n",
    "            profit = n_shares_sold * current_price\n",
    "            profit *= (1 - self.fee)\n",
    "            print('profit from selling', profit)\n",
    "            \n",
    "            self.balance += profit\n",
    "            self.shares_held -= n_shares_sold\n",
    "            print('balance', self.balance, 'shares held', self.shares_held)\n",
    "            \n",
    "        if action == 0:\n",
    "            self.balance = self.balance\n",
    "            self.shares_held = self.shares_held\n",
    "            print('balance', self.balance, 'shares held', self.shares_held)\n",
    "            \n",
    "        self.net_worth_prev = self.net_worth\n",
    "        self.net_worth = self.balance + (self.shares_held*current_price)\n",
    "        print('previous net worth', self.net_worth_prev, 'current net worth', self.net_worth)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _take_action_nonverbose(self, action):\n",
    "        \n",
    "        current_price = self.df.loc[self.current_step, 'close']\n",
    "\n",
    "        if action > 0:\n",
    "            total_possible = self.balance / (current_price*(1+self.fee))\n",
    "            n_shares_bought = total_possible * action\n",
    "                        \n",
    "            cost = n_shares_bought * current_price\n",
    "            cost *= (1 + self.fee)\n",
    "            \n",
    "            self.balance -= cost\n",
    "            self.shares_held += n_shares_bought\n",
    "            \n",
    "        if action < 0:\n",
    "            total_possible = self.shares_held * 1\n",
    "            n_shares_sold = total_possible * -action\n",
    "                        \n",
    "            profit = n_shares_sold * current_price\n",
    "            profit *= (1 - self.fee)\n",
    "            \n",
    "            self.balance += profit\n",
    "            self.shares_held -= n_shares_sold\n",
    "            \n",
    "        if action == 0:\n",
    "            self.balance = self.balance\n",
    "            self.shares_held = self.shares_held\n",
    "            \n",
    "        self.net_worth = self.balance + (self.shares_held*current_price)\n",
    "        \n",
    "        return self\n",
    "                \n",
    "    def _reward_fn(self):\n",
    "\n",
    "        current_price = self.df.loc[self.current_step, 'close']\n",
    "        total_possible = self.balance_long / (current_price*(1+self.fee))\n",
    "        cost = total_possible * current_price\n",
    "        cost *= (1+self.fee)\n",
    "\n",
    "        self.balance_long -= cost\n",
    "        self.shares_held_long += total_possible\n",
    "        self.net_worth_long = self.balance_long + (self.shares_held_long*current_price)\n",
    "        \n",
    "        profit = self.net_worth - self.balance_init\n",
    "        profit_long = self.net_worth_long - self.balance_init\n",
    "\n",
    "        return (profit - profit_long) / self.balance_init\n",
    "    \n",
    "    def step(self, action):\n",
    "                \n",
    "        if self.verbose > 0:\n",
    "            self._take_action_verbose(action)\n",
    "        elif self.verbose <= 0:\n",
    "            self._take_action_nonverbose(action)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # done\n",
    "        if self.training:\n",
    "            done = (self.balance < 0 or self.current_step >= self.train_test_split)\n",
    "        else:\n",
    "            done = (self.balance < 0 or self.current_step <= self.df.index[-2])\n",
    "           \n",
    "        # obs\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        # reward\n",
    "        reward = self._reward_fn()\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "            \n",
    "    def reset(self):\n",
    "        \n",
    "        if self.training: \n",
    "            self.current_step = np.random.randint(5, self.train_test_split-10)\n",
    "        else: \n",
    "            self.current_step = self.train_test_split\n",
    "        self.first_step = self.current_step\n",
    "        \n",
    "        self.net_worth = self.balance_init\n",
    "        self.balance = self.balance_init\n",
    "        self.shares_held = 0\n",
    "        \n",
    "        self.net_worth_long = self.balance_init\n",
    "        self.balance_long = self.balance_init\n",
    "        self.shares_held_long = 0\n",
    "    \n",
    "        return self._next_observation()\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \n",
    "        print('Current balance:', self.balance)\n",
    "        print('Current net worth:', self.net_worth)\n",
    "        print('Shares held:', self.shares_held)\n",
    "        print('Profit:', self.net_worth - self.balance_init)\n",
    "        print('Day range:', self.first_step, self.current_step)\n",
    "        print('Today:', self.df.loc[self.current_step], sep='\\n')\n",
    "        \n",
    "    def close(self):\n",
    "        \n",
    "        pass\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to build the model' neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_action_network(input_shape):\n",
    "    \n",
    "    h1 = 1528\n",
    "    h2 = 1032\n",
    "    eta = 0.01\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add( # Hidden layer 1\n",
    "        Dense(\n",
    "            units=h1, \n",
    "            activation='relu',\n",
    "            input_shape=(input_shape,),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add( # Hidden layer 2\n",
    "        Dense(\n",
    "            units=h2, \n",
    "            activation='relu',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add( # Output layer\n",
    "        Dense(\n",
    "            units=1, \n",
    "            activation='tanh',\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=Adam(lr=eta),\n",
    "        )\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_value_network(input_shape, output_len):\n",
    "        \n",
    "    h1 = 1016\n",
    "    h2 = 968\n",
    "    eta = 0.01\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add( # Input layer\n",
    "        Flatten(\n",
    "            input_shape=input_shape,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add( # Hidden layer 1\n",
    "        Dense(\n",
    "            units=h1, \n",
    "            activation='relu',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add( # Hidden layer 2\n",
    "        Dense(\n",
    "            units=h2, \n",
    "            activation='relu',\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add( # Output layer\n",
    "        Dense(\n",
    "            units=output_len, \n",
    "            activation='linear',\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=Adam(lr=eta),\n",
    "        )\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Double DQN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQN:\n",
    "    \n",
    "    def __init__(self, state_space, embeddings, \n",
    "        gamma=0.95, memory_size=1_000_000, batch_size=64, alpha=1.0, alpha_min=0.01, alpha_decay=0.99,\n",
    "                ):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.alpha_min = alpha_min\n",
    "        self.alpha_decay = alpha_decay\n",
    "        \n",
    "        self.state_space = state_space\n",
    "        self.embeddings = embeddings\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.value_network = build_value_network(state_space, len(embeddings))\n",
    "        self.action_network = build_action_network(len(embeddings))\n",
    "        \n",
    "        self.is_fit = False\n",
    "        self.verbose = 0\n",
    "        \n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() < self.alpha:\n",
    "            action = np.random.uniform(-1, 1)\n",
    "        \n",
    "        else:\n",
    "            if self.is_fit:\n",
    "                q_values = self.value_network.predict(\n",
    "                    state.astype(float).reshape(1, *self.state_space)\n",
    "                    )\n",
    "                action = self.action_network.predict(\n",
    "                    q_values.astype(float)\n",
    "                    )[0][0]\n",
    "\n",
    "            else:\n",
    "                q_values = [np.random.randn(len(self.embeddings))]\n",
    "                action = self.embeddings[\n",
    "                    np.argmax(q_values[0])\n",
    "                    ]\n",
    "\n",
    "        return action\n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, reward, state_next, terminal):\n",
    "        \n",
    "        self.memory.append(\n",
    "            (state, action, reward, state_next, terminal)\n",
    "            )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def replay(self):\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "            self.alpha = max(self.alpha_min, self.alpha*self.alpha_decay)\n",
    "            return None\n",
    "        \n",
    "        X, Y, a = [], [], []\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        for state, action, reward, state_next, terminal in mini_batch:\n",
    "            \n",
    "            a.append(action)\n",
    "            action_to_embedding = {e:np.abs(action-e) for e in self.embeddings}\n",
    "            action = int(np.where(\n",
    "                self.embeddings==min(\n",
    "                    action_to_embedding, \n",
    "                    key=action_to_embedding.get,\n",
    "                    )\n",
    "                )[0])\n",
    "            \n",
    "            if terminal:\n",
    "                q_update = reward\n",
    "            \n",
    "            else:\n",
    "                if self.is_fit:\n",
    "                    q_update = reward + self.gamma*np.amax(\n",
    "                        self.value_network.predict(state_next.astype(float).reshape(1, *self.state_space))[0]\n",
    "                        )\n",
    "                else:\n",
    "                    q_update = reward\n",
    "            \n",
    "            if self.is_fit:\n",
    "                q_values = self.value_network.predict(state.astype(float).reshape(1, *self.state_space))\n",
    "            else:\n",
    "                q_values = np.zeros((1, len(self.embeddings)))\n",
    "                \n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            X.append(state)\n",
    "            Y.append(q_values[0])\n",
    "            \n",
    "        X = np.array(X).reshape(self.batch_size, *self.state_space)\n",
    "        Y = np.array(Y).reshape(self.batch_size, len(self.embeddings))\n",
    "        a = np.array(a).reshape(self.batch_size, 1)\n",
    "        \n",
    "        batch = max(8, int(self.batch_size/8))\n",
    "        self.value_network.fit(\n",
    "            X.astype(float), \n",
    "            Y.astype(float), \n",
    "            batch_size=batch, \n",
    "            epochs=25, \n",
    "            verbose=self.verbose,\n",
    "            )\n",
    "        \n",
    "        values = self.value_network.predict(X.astype(float))\n",
    "        \n",
    "        self.action_network.fit(\n",
    "            values.astype(float),\n",
    "            a.astype(float),\n",
    "            batch_size=batch,\n",
    "            epochs=25,\n",
    "            verbose=self.verbose)\n",
    "        \n",
    "        self.is_fit = True\n",
    "        self.alpha = max(self.alpha_min, self.alpha*self.alpha_decay)\n",
    "        \n",
    "        return values, a\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and initialize environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_data('nflx_df')\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(None)\n",
    "\n",
    "starting_balance = 1_000_000\n",
    "\n",
    "env = TradingEnv(df, balance_init=starting_balance)\n",
    "env.verbose=0\n",
    "env.seed(None)\n",
    "\n",
    "print(env.action_space, env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Double DQN agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver = DoubleDQN(\n",
    "    state_space=env.observation_space.shape, \n",
    "    embeddings=np.linspace(-1,1,9),\n",
    "    batch_size=64,\n",
    "    memory_size=1_000_000,\n",
    "    alpha=0.99,\n",
    "    alpha_decay=0.9,\n",
    "    alpha_min=0.01,\n",
    "    )\n",
    "\n",
    "dqn_solver.verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver.value_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver.action_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training by repeating backtests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(100):\n",
    "    \n",
    "    # Reset the evironment at the top of each episode\n",
    "    state = env.reset()\n",
    "    \n",
    "    stock_performance = []\n",
    "    model_performance = []\n",
    "    reward_trace = []    \n",
    "    actions = []\n",
    "    \n",
    "    start = env.current_step\n",
    "    done = False    \n",
    "    \n",
    "    # The model will iterate until a terminal state is reached\n",
    "    while not done:\n",
    "                \n",
    "        # Select an action by passing the current observation/state to the DQN\n",
    "        action = dqn_solver.act(state)\n",
    "        \n",
    "        # The environment takes a step according to that action and returns the new state, the reward, and the terminal status\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "        # Commit to the DQN's memory the relevant information\n",
    "        dqn_solver.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        actions.append(action)\n",
    "        reward_trace.append(reward)\n",
    "        model_performance.append(env.net_worth)  \n",
    "        stock_performance.append(df.loc[env.current_step]['close'])\n",
    "            \n",
    "    print('================================================================================================================')\n",
    "    print(i+1)\n",
    "    print('TOTAL REWARD:', np.mean(reward_trace))\n",
    "    print('DAY RANGE:', start, env.current_step)\n",
    "    print('EXPLORATION:', dqn_solver.alpha)\n",
    "    print('MEMORY SIZE:', len(dqn_solver.memory))\n",
    "    print('================================================================================================================')\n",
    "    \n",
    "    # After each episode, perform experience replay\n",
    "    test = dqn_solver.replay()  \n",
    "    \n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    history.append({\n",
    "        'stock': stock_performance,\n",
    "        'model': model_performance,\n",
    "        'actions': actions,\n",
    "        'rewards': np.mean(reward_trace),\n",
    "        })\n",
    "\n",
    "X, y = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate buy and hold performance for given training instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = buy_and_hold(\n",
    "    balance_init=env.balance_init,\n",
    "    back_prices=history[i]['stock'],\n",
    "    fee=env.fee\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model performance and buy and hold performance for given instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "ax1.plot(long, 'b--', alpha=0.667, label='buy and hold')\n",
    "ax1.plot(history[i]['model'], 'b-', label='backtesting model')\n",
    "ax1.axhline(env.balance_init, alpha=0.333, color='blue')\n",
    "ax1.set_ylabel('portfolio value')\n",
    "ax1.set_title('training')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history[i]['stock'], 'r-', label='stock history')\n",
    "ax2.set_ylabel('stock value')\n",
    "ax2.set_xlabel('time')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(history[i]['actions'], 'b-', alpha=0.6667)\n",
    "plt.plot(history[i]['actions'], 'r.')\n",
    "\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('portion of portfolio bought/sold')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot average reward during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([history[i]['rewards'] for i in np.arange(len(history))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(rewards[:-8], 'b-')\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('total reward per episode')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find correlation between prices and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(history[i]['stock'], history[i]['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for testing a DQN agent with a two hidden layer neural network in a backtesting environment. Agents/models built in this notebook are the main models \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from support_code.model import DQN\n",
    "from support_code.env import TradingEnv\n",
    "from support_code.functions import fetch_data, buy_and_hold, pickle_model, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "### Random back testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and initialize environment for backtesting a random-action agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_data('sp500_df')\n",
    "\n",
    "starting_balance = 1_000_000\n",
    "\n",
    "env = TradingEnv(df, balance_init=starting_balance)\n",
    "env.verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(None)\n",
    "env.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a round of backtesting where actions are chosen at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Perform a round of backtesting where actions are chosen at random:done = False\n",
    "obs = env.reset()\n",
    "start = env.current_step\n",
    "\n",
    "stock_performance = []\n",
    "model_performance = []\n",
    "actions = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    stock_performance.append(df.loc[env.current_step]['close'])\n",
    "    model_performance.append(env.net_worth)\n",
    "        \n",
    "    action = np.random.randint(env.action_space.n)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "\n",
    "    actions.append(env._actions[action])\n",
    "    \n",
    "end = env.current_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot performance of random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot performance of random agent:fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "ax1.plot(model_performance, 'b-', label='stochastic model')\n",
    "ax1.set_ylabel('portfolio value')\n",
    "ax1.axhline(starting_balance, alpha=0.5, color='blue')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(stock_performance, 'r-', label='stock history')\n",
    "ax2.set_ylabel('stock value')\n",
    "ax2.set_xlabel('time')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot the random actions:plt.figure(figsize=(15,5))\n",
    "plt.plot(actions, 'r.')\n",
    "plt.plot(actions, 'b-')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('portion of portfolio bought/sold')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the correlation between price and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find the correlation between price and actionscorr(actions, stock_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "### Training/backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize environment for training DQN agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_data('sp500_df')\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(None)\n",
    "\n",
    "starting_balance = 1_000_000\n",
    "\n",
    "env = TradingEnv(df, balance_init=starting_balance)\n",
    "env.verbose=0\n",
    "env.seed(None)\n",
    "\n",
    "#print(env.action_space, env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize DQN agent with two hidden layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize DQN agent with three hidden layer neural network:dqn_solver = DQN(\n",
    "    action_space=env.action_space.n, \n",
    "    state_space=env.observation_space.shape,\n",
    "    batch_size=64,\n",
    "    memory_size=1_000_000,\n",
    "    alpha=1.0,\n",
    "    alpha_decay=0.999,\n",
    "    alpha_min=0.1,\n",
    "    gamma=0.99,\n",
    "    )\n",
    "\n",
    "dqn_solver.verbose = 0\n",
    "#dqn_solver.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training by repeating backtests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Perform training by repeating backtests:for i in np.arange(750, 2500):\n",
    "    \n",
    "    # Reset the evironment at the top of each episode\n",
    "    state = env.reset()\n",
    "    \n",
    "    stock_performance = []\n",
    "    model_performance = []\n",
    "    reward_trace = []    \n",
    "    actions = []\n",
    "    \n",
    "    start = env.current_step\n",
    "    done = False    \n",
    "    \n",
    "    # The model will iterate until a terminal state is reached\n",
    "    while not done:\n",
    "                \n",
    "        # Select an action by passing the current observation/state to the DQN\n",
    "        action = dqn_solver.act(state)\n",
    "        \n",
    "        # The environment takes a step according to that action and returns the new state, the reward, and the terminal status\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "        # Commit to the DQN's memory the relevant information\n",
    "        dqn_solver.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "\n",
    "        actions.append(action)\n",
    "        reward_trace.append(reward)\n",
    "        model_performance.append(env.net_worth)  \n",
    "        stock_performance.append(df.loc[env.current_step]['close'])\n",
    "            \n",
    "    print('================================================================================================================')\n",
    "    print(i+1)\n",
    "    print('FINAL PROFIT', env.net_worth-env.balance_init)\n",
    "    print('TOTAL REWARD:', np.mean(reward_trace))\n",
    "    print('DAY RANGE:', start, env.current_step)\n",
    "    print('EXPLORATION:', dqn_solver.alpha)\n",
    "    print('MEMORY SIZE:', len(dqn_solver.memory))\n",
    "    print('================================================================================================================')\n",
    "    \n",
    "    # After each episode, perform experience replay\n",
    "    test = dqn_solver.replay()  \n",
    "    \n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    history.append({\n",
    "        'stock': stock_performance,\n",
    "        'model': model_performance,\n",
    "        'actions': actions,\n",
    "        'rewards': np.mean(reward_trace),\n",
    "        })\n",
    "\n",
    "X, y = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate buy and hold performance for given training instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate buy and hold performance for given training instance:long = buy_and_hold(\n",
    "    balance_init=env.balance_init,\n",
    "    back_prices=history[i]['stock'],\n",
    "    fee=env.fee\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model performance and buy and hold performance for given instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot model performance and buy and hold performance for given instance:fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "ax1.plot(long, 'b--', alpha=0.667, label='buy and hold')\n",
    "ax1.plot(history[i]['model'], 'b-', label='backtesting model')\n",
    "ax1.axhline(env.balance_init, alpha=0.333, color='blue')\n",
    "ax1.set_ylabel('portfolio value')\n",
    "ax1.set_title('training')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history[i]['stock'], 'r-', label='stock history')\n",
    "ax2.set_ylabel('stock value')\n",
    "ax2.set_xlabel('time')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot actions:plt.figure(figsize=(15,5))\n",
    "plt.plot(history[i]['actions'], 'b-', alpha=0.6667)\n",
    "plt.plot(history[i]['actions'], 'r.')\n",
    "\n",
    "plt.yticks(np.arange(dqn_solver.action_space), np.linspace(-1,1, dqn_solver.action_space))\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('portion of portfolio bought/sold')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot average reward during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot average reward during training:rewards = np.array([history[i]['rewards'] for i in np.arange(len(history))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find correlation between prices and actions:plt.figure(figsize=(12,6))\n",
    "plt.plot(rewards, 'b-')\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('total reward per episode')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find correlation between prices and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(history[i]['stock'], history[i]['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the modle for further testing and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_model(dqn_solver, path='model_info_sp500_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
